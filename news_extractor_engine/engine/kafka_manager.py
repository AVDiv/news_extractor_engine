import json
import logging
import os
import threading
from typing import Any, Dict, Optional

from confluent_kafka import Producer


class KafkaProducerManager:
    """
    Manages a pool of Kafka producers for multiple worker threads
    """

    __KAFKA_BOOTSTRAP_SERVERS: str
    __KAFKA_CLIENT_ID_PREFIX: str
    __KAFKA_PRODUCER_TOPIC: str
    __KAFKA_CONFIG: Dict[str, Any]

    def __init__(self, num_producers: int = 3):
        """
        Initialize the Kafka producer manager

        Args:
            num_producers: Number of producers to initialize
        """
        # Kafka configuration
        self.__KAFKA_BOOTSTRAP_SERVERS: str = os.getenv(
            "KAFKA_BOOTSTRAP_SERVERS", "localhost:9092"
        )
        self.__KAFKA_CLIENT_ID_PREFIX: str = (
            os.getenv("KAFKA_PRODUCER_CLIENT_ID_PREFIX", "news-extractor")
            if os.getenv("KAFKA_PRODUCER_CLIENT_ID_AUTOGENERATED") != "true"
            else "news-extractor-engine"
        )
        self.__KAFKA_PRODUCER_TOPIC: str = os.getenv("KAFKA_PRODUCER_TOPIC")
        self.__KAFKA_CONFIG: Dict[str, Any] = {
            "bootstrap.servers": self.__KAFKA_BOOTSTRAP_SERVERS,
            "client.id": self.__KAFKA_CLIENT_ID_PREFIX,
        }

        self.lock = threading.RLock()
        self.producers = {}
        self.producer_usage = {}

        logging.info("Connecting to Kafka server on %s", self.__KAFKA_BOOTSTRAP_SERVERS)

        # Initialize producers
        for i in range(num_producers):
            producer_id = f"producer-{i}"
            self.__KAFKA_CONFIG["client.id"] = (
                f"{self.__KAFKA_CLIENT_ID_PREFIX}-producer-{i}"
            )
            self.producers[producer_id] = Producer(self.__KAFKA_CONFIG)
            self.producer_usage[producer_id] = False

    def get_producer(self) -> tuple[str, Producer]:
        """
        Get an available producer from the pool

        Returns:
            Tuple of (producer_id, producer)
        """
        with self.lock:
            for producer_id, in_use in self.producer_usage.items():
                if not in_use:
                    self.producer_usage[producer_id] = True
                    return producer_id, self.producers[producer_id]

            # If all producers are in use, create a new one
            new_producer_id = f"producer-{len(self.producers)}"
            new_producer = Producer(self._default_config)
            self.producers[new_producer_id] = new_producer
            self.producer_usage[new_producer_id] = True
            return new_producer_id, new_producer

    def release_producer(self, producer_id: str):
        """
        Mark a producer as available

        Args:
            producer_id: ID of the producer to release
        """
        with self.lock:
            if producer_id in self.producer_usage:
                self.producer_usage[producer_id] = False

    def publish_message(self, key: str, value: dict) -> bool:
        """
        Publish a message to Kafka

        Args:
            key: Message key
            value: Message value (will be converted to JSON)

        Returns:
            True if message was sent successfully, False otherwise
        """
        producer_id, producer = self.get_producer()
        try:
            encoded_value = json.dumps(value).encode("utf-8")
            producer.produce(
                self.__KAFKA_PRODUCER_TOPIC,
                key=key,
                value=encoded_value,
                callback=lambda err, msg: self._delivery_callback(
                    err, msg, producer_id
                ),
            )
            producer.poll(0)  # Trigger any callbacks
            return True
        except Exception as e:
            logging.error(f"Failed to publish message to Kafka: {str(e)}")
            self.release_producer(producer_id)
            return False

    def _delivery_callback(self, err, msg, producer_id):
        """
        Callback for Kafka message delivery reports
        """
        if err:
            logging.error(f"Message delivery failed: {err}")
        else:
            logging.debug(
                f"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}"
            )

        # Release the producer for reuse
        self.release_producer(producer_id)

    def flush_all(self):
        """Flush all producers"""
        for producer in self.producers.values():
            producer.flush()

    def close_all(self):
        """Close all producers"""
        for producer in self.producers.values():
            producer.flush()
