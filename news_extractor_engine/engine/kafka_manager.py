import json
import logging
import os
import threading
from typing import Any, Dict, Optional

from confluent_kafka import Producer


class KafkaProducerManager:
    """
    Manages a pool of Kafka producers for multiple worker threads
    """

    __KAFKA_BOOTSTRAP_SERVERS: str
    __KAFKA_CLIENT_ID_PREFIX: str
    __KAFKA_PRODUCER_TOPIC: str
    __KAFKA_AUTH_ENABLED: bool
    __KAFKA_CONFIG: Dict[str, Any]

    def __init__(self, num_producers: int = 3):
        """
        Initialize the Kafka producer manager

        Args:
            num_producers: Number of producers to initialize
        """
        # Kafka configuration
        self.__KAFKA_BOOTSTRAP_SERVERS: str = os.getenv(
            "KAFKA_BOOTSTRAP_SERVERS", "localhost:9092"
        )
        self.__KAFKA_CLIENT_ID_PREFIX: str = (
            os.getenv("KAFKA_PRODUCER_CLIENT_ID_PREFIX", "news-extractor")
            if os.getenv("KAFKA_PRODUCER_CLIENT_ID_AUTOGENERATED") != "true"
            else "news-extractor-engine"
        )
        self.__KAFKA_PRODUCER_TOPIC: str = os.getenv(
            "KAFKA_PRODUCER_TOPIC", "news_articles"
        )
        self.__KAFKA_CONFIG: Dict[str, Any] = {
            "bootstrap.servers": self.__KAFKA_BOOTSTRAP_SERVERS,
            "client.id": self.__KAFKA_CLIENT_ID_PREFIX,
        }
        self.__KAFKA_AUTH_ENABLED: bool = (
            os.getenv("KAFKA_AUTH_ENABLED", "false").lower() == "true"
        )
        if self.__KAFKA_AUTH_ENABLED:
            self.__KAFKA_CONFIG["sasl.username"] = os.getenv("KAFKA_AUTH_USERNAME", "")
            self.__KAFKA_CONFIG["sasl.password"] = os.getenv("KAFKA_AUTH_PASSWORD", "")
            self.__KAFKA_CONFIG["sasl.mechanism"] = os.getenv(
                "KAFKA_AUTH_MECHANISM", "PLAIN"
            )
            self.__KAFKA_CONFIG["security.protocol"] = os.getenv(
                "KAFKA_SECURITY_PROTOCOL", "PLAINTEXT"
            )

        self.lock = threading.RLock()
        self.producers = {}
        self.producer_usage = {}

        logging.info("Connecting to Kafka server on %s", self.__KAFKA_BOOTSTRAP_SERVERS)

        # Initialize producers
        for i in range(num_producers):
            producer_id = f"producer-{i}"
            config = self.__KAFKA_CONFIG.copy()
            config["client.id"] = f"{self.__KAFKA_CLIENT_ID_PREFIX}-producer-{i}"
            self.producers[producer_id] = Producer(config)
            self.producer_usage[producer_id] = False

    def get_producer(self) -> tuple[str, Producer]:
        """
        Get an available producer from the pool

        Returns:
            Tuple of (producer_id, producer)
        """
        with self.lock:
            for producer_id, in_use in self.producer_usage.items():
                if not in_use:
                    self.producer_usage[producer_id] = True
                    return producer_id, self.producers[producer_id]

            # If all producers are in use, create a new one
            new_producer_id = f"producer-{len(self.producers)}"
            config = self.__KAFKA_CONFIG.copy()
            config["client.id"] = f"{self.__KAFKA_CLIENT_ID_PREFIX}-{new_producer_id}"
            new_producer = Producer(config)
            self.producers[new_producer_id] = new_producer
            self.producer_usage[new_producer_id] = True
            logging.info(
                f"Created new producer {new_producer_id} as all existing producers are busy"
            )
            return new_producer_id, new_producer

    def release_producer(self, producer_id: str):
        """
        Mark a producer as available

        Args:
            producer_id: ID of the producer to release
        """
        with self.lock:
            if producer_id in self.producer_usage:
                self.producer_usage[producer_id] = False
                logging.debug(f"Released producer {producer_id}")
            else:
                logging.warning(f"Attempted to release unknown producer: {producer_id}")

    def publish_message(self, key: str, value: dict) -> bool:
        """
        Publish a message to Kafka

        Args:
            key: Message key
            value: Message value (will be converted to JSON)

        Returns:
            True if message was sent successfully, False otherwise
        """
        producer_id = None
        try:
            producer_id, producer = self.get_producer()
            encoded_value = json.dumps(value).encode("utf-8")

            # Set up delivery tracking
            delivery_success = [False]
            delivery_complete = threading.Event()

            def delivery_callback(err, msg):
                if err:
                    logging.error(f"Message delivery failed: {err}")
                else:
                    logging.debug(
                        f"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}"
                    )
                    delivery_success[0] = True

                # Release the producer for reuse
                self.release_producer(producer_id)
                delivery_complete.set()

            producer.produce(
                self.__KAFKA_PRODUCER_TOPIC,
                key=key,
                value=encoded_value,
                callback=delivery_callback,
            )

            # Trigger any callbacks and wait briefly for delivery
            producer.poll(0.1)

            # If delivery was immediate, wait for callback to complete
            if not delivery_complete.is_set():
                # Don't wait indefinitely - this is just to give a chance for immediate delivery
                delivery_complete.wait(0.5)

            # Return based on delivery result - if we didn't get a callback yet,
            # we'll still consider it "sent" since it's in Kafka's queue
            return True

        except Exception as e:
            logging.error(f"Failed to publish message to Kafka: {str(e)}")
            if producer_id:
                self.release_producer(producer_id)
            return False

    def flush_all(self):
        """Flush all producers"""
        for producer in self.producers.values():
            try:
                producer.flush(timeout=5.0)
            except Exception as e:
                logging.error(f"Error flushing producer: {e}")

    def close_all(self):
        """Close all producers"""
        with self.lock:
            for producer_id, producer in list(self.producers.items()):
                try:
                    producer.flush(timeout=5.0)
                    # Explicitly remove the producer
                    del self.producers[producer_id]
                    if producer_id in self.producer_usage:
                        del self.producer_usage[producer_id]
                except Exception as e:
                    logging.error(f"Error closing producer {producer_id}: {e}")

            # Clear the collections to ensure all references are removed
            self.producers.clear()
            self.producer_usage.clear()
            logging.info("All Kafka producers have been closed")
