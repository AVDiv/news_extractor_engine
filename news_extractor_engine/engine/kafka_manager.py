import json
import logging
import os
import threading
import time
from typing import Any, Dict, Optional, Tuple

from confluent_kafka import KafkaException, Producer


class KafkaProducerManager:
    """
    Manages a fixed-size pool of Kafka producers with failure handling
    """

    __KAFKA_BOOTSTRAP_SERVERS: str
    __KAFKA_CLIENT_ID_PREFIX: str
    __KAFKA_PRODUCER_TOPIC: str
    __KAFKA_AUTH_ENABLED: bool
    __KAFKA_CONFIG: Dict[str, Any]
    __MAX_PRODUCERS: int = (
        10  # Reduced from 50 to 10 to prevent thread resource exhaustion
    )
    __PRODUCER_ACQUIRE_TIMEOUT: float = 2.0  # Reduced timeout to detect issues faster
    __MAX_PRODUCER_RETRIES: int = (
        3  # Maximum number of retries when creating a producer
    )
    __RETRY_BACKOFF_BASE: float = 1.0  # Base backoff time in seconds
    __RETRY_BACKOFF_MAX: float = 30.0  # Maximum backoff time in seconds
    __THREAD_CREATION_ERRORS: int = 0  # Count of thread creation errors

    def __init__(self, num_producers: int = 5):
        """
        Initialize the Kafka producer manager with a fixed pool size

        Args:
            num_producers: Initial number of producers to initialize (will not exceed MAX_PRODUCERS)
        """
        # Kafka configuration
        self.__KAFKA_BOOTSTRAP_SERVERS: str = os.getenv(
            "KAFKA_BOOTSTRAP_SERVERS", "localhost:9092"
        )
        self.__KAFKA_CLIENT_ID_PREFIX: str = (
            os.getenv("KAFKA_PRODUCER_CLIENT_ID_PREFIX", "news-extractor")
            if os.getenv("KAFKA_PRODUCER_CLIENT_ID_AUTOGENERATED") != "true"
            else "news-extractor-engine"
        )
        self.__KAFKA_PRODUCER_TOPIC: str = os.getenv(
            "KAFKA_PRODUCER_TOPIC", "news_articles"
        )

        # Enhanced Kafka client configuration with thread management settings
        self.__KAFKA_CONFIG: Dict[str, Any] = {
            "bootstrap.servers": self.__KAFKA_BOOTSTRAP_SERVERS,
            "client.id": self.__KAFKA_CLIENT_ID_PREFIX,
            # Add socket timeout to prevent hanging
            "socket.timeout.ms": 10000,  # 10 second socket timeout
            # Maximum number of threads the client will use
            "internal.max.threads": 2,  # Limit internal threads per producer
            # Message batching to reduce thread usage
            "batch.num.messages": 500,  # Increase batch size
            "linger.ms": 50,  # Wait up to 50ms to batch messages
            # Set queue buffering to handle back pressure
            "queue.buffering.max.messages": 100000,
            "queue.buffering.max.ms": 100,
            # Set message delivery timeout
            "delivery.timeout.ms": 30000,  # 30 seconds
            # Background poll configuration
            "background.poll.interval.ms": 500,  # More frequent polling
        }

        self.__KAFKA_AUTH_ENABLED: bool = (
            os.getenv("KAFKA_AUTH_ENABLED", "false").lower() == "true"
        )
        if self.__KAFKA_AUTH_ENABLED:
            self.__KAFKA_CONFIG["sasl.username"] = os.getenv("KAFKA_AUTH_USERNAME", "")
            self.__KAFKA_CONFIG["sasl.password"] = os.getenv("KAFKA_AUTH_PASSWORD", "")
            self.__KAFKA_CONFIG["sasl.mechanism"] = os.getenv(
                "KAFKA_AUTH_MECHANISM", "PLAIN"
            )
            self.__KAFKA_CONFIG["security.protocol"] = os.getenv(
                "KAFKA_SECURITY_PROTOCOL", "PLAINTEXT"
            )

        self.lock = threading.RLock()
        self.producers = {}
        self.producer_usage = {}
        self.producer_condition = threading.Condition(self.lock)
        self.producer_creation_errors = {}  # Track creation errors per producer
        self.fallback_mode = False  # Flag to indicate we're in fallback mode

        logging.info("Connecting to Kafka server on %s", self.__KAFKA_BOOTSTRAP_SERVERS)

        # Cap the initial number of producers to the maximum allowed
        initial_producers = min(num_producers, self.__MAX_PRODUCERS)
        logging.info(
            f"Initializing pool with {initial_producers} Kafka producers (max: {self.__MAX_PRODUCERS})"
        )

        # Initialize producers with error handling
        successful_producers = 0
        for i in range(initial_producers):
            try:
                producer_id = f"producer-{i}"
                config = self.__KAFKA_CONFIG.copy()
                config["client.id"] = f"{self.__KAFKA_CLIENT_ID_PREFIX}-producer-{i}"

                # Try to create the producer with retries
                producer = self._create_producer_with_retry(config)
                if producer:
                    self.producers[producer_id] = producer
                    self.producer_usage[producer_id] = False
                    successful_producers += 1
            except Exception as e:
                logging.error(f"Failed to initialize producer {i}: {str(e)}")

        logging.info(
            f"Successfully initialized {successful_producers} of {initial_producers} requested producers"
        )

        if successful_producers == 0:
            logging.critical(
                "CRITICAL: Could not create any Kafka producers. Messages will be lost!"
            )
            self.fallback_mode = True

    def _create_producer_with_retry(self, config: Dict[str, Any]) -> Optional[Producer]:
        """
        Attempt to create a producer with retries and exponential backoff

        Args:
            config: Kafka producer configuration

        Returns:
            Producer instance if successful, None otherwise
        """
        retry_count = 0
        while retry_count < self.__MAX_PRODUCER_RETRIES:
            try:
                return Producer(config)
            except KafkaException as e:
                retry_count += 1
                if "Unable to create broker thread" in str(e):
                    # This is a thread creation error - track it globally
                    self.__class__.__THREAD_CREATION_ERRORS += 1
                    logging.error(
                        f"Thread creation error: {str(e)}. Error count: {self.__class__.__THREAD_CREATION_ERRORS}"
                    )

                    # If we've seen this error multiple times, reduce our pool size
                    if (
                        self.__class__.__THREAD_CREATION_ERRORS > 5
                        and self.__class__.__MAX_PRODUCERS > 5
                    ):
                        old_max = self.__class__.__MAX_PRODUCERS
                        self.__class__.__MAX_PRODUCERS = max(
                            5, self.__class__.__MAX_PRODUCERS - 3
                        )
                        logging.warning(
                            f"Reducing max producers from {old_max} to {self.__class__.__MAX_PRODUCERS} due to thread creation errors"
                        )

                # Calculate backoff time with exponential backoff
                backoff_time = min(
                    self.__RETRY_BACKOFF_MAX,
                    self.__RETRY_BACKOFF_BASE * (2 ** (retry_count - 1)),
                )

                if retry_count < self.__MAX_PRODUCER_RETRIES:
                    logging.warning(
                        f"Producer creation attempt {retry_count} failed: {str(e)}. Retrying in {backoff_time:.1f}s..."
                    )
                    time.sleep(backoff_time)
                else:
                    logging.error(
                        f"Failed to create producer after {self.__MAX_PRODUCER_RETRIES} attempts: {str(e)}"
                    )
            except Exception as e:
                logging.error(f"Unexpected error creating producer: {str(e)}")
                retry_count += 1
                if retry_count < self.__MAX_PRODUCER_RETRIES:
                    time.sleep(1)  # Simple delay for unexpected errors
                else:
                    break

        return None

    def get_producer(self) -> Tuple[str, Optional[Producer]]:
        """
        Get an available producer from the pool.
        If all producers are in use, wait for one to become available.

        Returns:
            Tuple of (producer_id, producer) or (None, None) if in fallback mode

        Raises:
            TimeoutError: If no producer becomes available within the timeout period
        """
        # Quick check for fallback mode
        if self.fallback_mode:
            return None, None

        with self.producer_condition:
            # First attempt: try to find an available producer
            for producer_id, in_use in self.producer_usage.items():
                if not in_use:
                    self.producer_usage[producer_id] = True
                    return producer_id, self.producers[producer_id]

            # If we have fewer than MAX_PRODUCERS, create a new one
            if len(self.producers) < self.__class__.__MAX_PRODUCERS:
                try:
                    new_producer_id = f"producer-{len(self.producers)}"
                    config = self.__KAFKA_CONFIG.copy()
                    config["client.id"] = (
                        f"{self.__KAFKA_CLIENT_ID_PREFIX}-{new_producer_id}"
                    )

                    # Try to create a new producer
                    new_producer = self._create_producer_with_retry(config)
                    if new_producer:
                        self.producers[new_producer_id] = new_producer
                        self.producer_usage[new_producer_id] = True
                        logging.info(
                            f"Created new producer {new_producer_id} (total: {len(self.producers)}/{self.__class__.__MAX_PRODUCERS})"
                        )
                        return new_producer_id, new_producer
                    else:
                        logging.error(
                            f"Failed to create new producer {new_producer_id}"
                        )
                except Exception as e:
                    logging.error(f"Error creating new producer: {str(e)}")

            # All producers are in use and we're at the limit, wait for one to become available
            logging.warning(
                f"All {len(self.producers)} Kafka producers are in use. Waiting for one to become available..."
            )

            # Wait for a producer to become available with timeout
            start_time = time.time()
            while True:
                # Wait for a notification that a producer might be available
                self.producer_condition.wait(timeout=self.__PRODUCER_ACQUIRE_TIMEOUT)

                # Check if any producer is now available
                for producer_id, in_use in self.producer_usage.items():
                    if not in_use:
                        self.producer_usage[producer_id] = True
                        logging.debug(f"Acquired producer {producer_id} after waiting")
                        return producer_id, self.producers[producer_id]

                # Check if we've exceeded our timeout
                if time.time() - start_time > self.__PRODUCER_ACQUIRE_TIMEOUT:
                    logging.error(
                        f"Timeout waiting for an available Kafka producer. All {len(self.producers)} producers are busy."
                    )
                    raise TimeoutError(
                        f"Timed out waiting for an available Kafka producer"
                    )

    def release_producer(self, producer_id: str):
        """
        Mark a producer as available and notify waiting threads

        Args:
            producer_id: ID of the producer to release
        """
        if producer_id is None:
            return  # Nothing to do in fallback mode

        with self.producer_condition:
            if producer_id in self.producer_usage:
                self.producer_usage[producer_id] = False
                logging.debug(f"Released producer {producer_id}")
                # Notify one waiting thread that a producer is available
                self.producer_condition.notify()
            else:
                logging.warning(f"Attempted to release unknown producer: {producer_id}")

    def publish_message(self, key: str, value: dict) -> bool:
        """
        Publish a message to Kafka

        Args:
            key: Message key
            value: Message value (will be converted to JSON)

        Returns:
            True if message was sent successfully, False otherwise
        """
        producer_id = None
        try:
            # Get a producer with timeout handling
            producer_id, producer = self.get_producer()

            # Check if we're in fallback mode or couldn't get a producer
            if producer is None:
                logging.warning(
                    f"No Kafka producer available, message will not be sent: {key}"
                )
                return False

            encoded_value = json.dumps(value).encode("utf-8")

            # Set up delivery tracking
            delivery_success = [False]
            delivery_complete = threading.Event()

            def delivery_callback(err, msg):
                if err:
                    logging.error(f"Message delivery failed: {err}")
                else:
                    logging.debug(
                        f"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}"
                    )
                    delivery_success[0] = True

                # Release the producer for reuse
                self.release_producer(producer_id)
                delivery_complete.set()

            producer.produce(
                self.__KAFKA_PRODUCER_TOPIC,
                key=key,
                value=encoded_value,
                callback=delivery_callback,
            )

            # Trigger any callbacks and wait briefly for delivery
            producer.poll(0)  # Non-blocking poll

            # If delivery was immediate, wait for callback to complete
            if not delivery_complete.is_set():
                # Don't wait indefinitely - this is just to give a chance for immediate delivery
                delivery_complete.wait(0.2)  # Reduced wait time to improve throughput

            # Return based on delivery result - if we didn't get a callback yet,
            # we'll still consider it "sent" since it's in Kafka's queue
            return True

        except TimeoutError:
            logging.error(
                f"Timeout getting Kafka producer, message will not be sent: {key}"
            )
            return False
        except Exception as e:
            logging.error(f"Failed to publish message to Kafka: {str(e)}")
            if producer_id:
                self.release_producer(producer_id)
            return False

    def flush_all(self):
        """Flush all producers"""
        for producer in self.producers.values():
            try:
                producer.flush(timeout=2.0)  # Reduced timeout to prevent hanging
            except Exception as e:
                logging.error(f"Error flushing producer: {e}")

    def close_all(self):
        """Close all producers"""
        with self.lock:
            for producer_id, producer in list(self.producers.items()):
                try:
                    producer.flush(timeout=2.0)  # Reduced timeout to prevent hanging
                    # Explicitly remove the producer
                    del self.producers[producer_id]
                    if producer_id in self.producer_usage:
                        del self.producer_usage[producer_id]
                except Exception as e:
                    logging.error(f"Error closing producer {producer_id}: {e}")

            # Clear the collections to ensure all references are removed
            self.producers.clear()
            self.producer_usage.clear()
            logging.info("All Kafka producers have been closed")

    def get_producer_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the producer pool

        Returns:
            Dictionary with producer pool statistics
        """
        with self.lock:
            total_producers = len(self.producers)
            active_producers = sum(
                1 for in_use in self.producer_usage.values() if in_use
            )
            return {
                "total_producers": total_producers,
                "active_producers": active_producers,
                "available_producers": total_producers - active_producers,
                "max_producers": self.__class__.__MAX_PRODUCERS,
                "thread_errors": self.__class__.__THREAD_CREATION_ERRORS,
                "fallback_mode": self.fallback_mode,
            }
