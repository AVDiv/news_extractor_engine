import json
import logging
import os
import threading
import time
from typing import Any, Dict, Optional, Tuple

from confluent_kafka import Producer


class KafkaProducerManager:
    """
    Manages a fixed-size pool of Kafka producers for multiple worker threads
    """

    __KAFKA_BOOTSTRAP_SERVERS: str
    __KAFKA_CLIENT_ID_PREFIX: str
    __KAFKA_PRODUCER_TOPIC: str
    __KAFKA_AUTH_ENABLED: bool
    __KAFKA_CONFIG: Dict[str, Any]
    __MAX_PRODUCERS: int = 50  # Maximum number of producers to create
    __PRODUCER_ACQUIRE_TIMEOUT: float = (
        5.0  # Timeout in seconds to wait for an available producer
    )

    def __init__(self, num_producers: int = 10):
        """
        Initialize the Kafka producer manager with a fixed pool size

        Args:
            num_producers: Initial number of producers to initialize (will not exceed MAX_PRODUCERS)
        """
        # Kafka configuration
        self.__KAFKA_BOOTSTRAP_SERVERS: str = os.getenv(
            "KAFKA_BOOTSTRAP_SERVERS", "localhost:9092"
        )
        self.__KAFKA_CLIENT_ID_PREFIX: str = (
            os.getenv("KAFKA_PRODUCER_CLIENT_ID_PREFIX", "news-extractor")
            if os.getenv("KAFKA_PRODUCER_CLIENT_ID_AUTOGENERATED") != "true"
            else "news-extractor-engine"
        )
        self.__KAFKA_PRODUCER_TOPIC: str = os.getenv(
            "KAFKA_PRODUCER_TOPIC", "news_articles"
        )
        self.__KAFKA_CONFIG: Dict[str, Any] = {
            "bootstrap.servers": self.__KAFKA_BOOTSTRAP_SERVERS,
            "client.id": self.__KAFKA_CLIENT_ID_PREFIX,
        }
        self.__KAFKA_AUTH_ENABLED: bool = (
            os.getenv("KAFKA_AUTH_ENABLED", "false").lower() == "true"
        )
        if self.__KAFKA_AUTH_ENABLED:
            self.__KAFKA_CONFIG["sasl.username"] = os.getenv("KAFKA_AUTH_USERNAME", "")
            self.__KAFKA_CONFIG["sasl.password"] = os.getenv("KAFKA_AUTH_PASSWORD", "")
            self.__KAFKA_CONFIG["sasl.mechanism"] = os.getenv(
                "KAFKA_AUTH_MECHANISM", "PLAIN"
            )
            self.__KAFKA_CONFIG["security.protocol"] = os.getenv(
                "KAFKA_SECURITY_PROTOCOL", "PLAINTEXT"
            )

        self.lock = threading.RLock()
        self.producers = {}
        self.producer_usage = {}
        self.producer_condition = threading.Condition(self.lock)

        logging.info("Connecting to Kafka server on %s", self.__KAFKA_BOOTSTRAP_SERVERS)

        # Cap the initial number of producers to the maximum allowed
        initial_producers = min(num_producers, self.__MAX_PRODUCERS)
        logging.info(
            f"Initializing pool with {initial_producers} Kafka producers (max: {self.__MAX_PRODUCERS})"
        )

        # Initialize producers
        for i in range(initial_producers):
            producer_id = f"producer-{i}"
            config = self.__KAFKA_CONFIG.copy()
            config["client.id"] = f"{self.__KAFKA_CLIENT_ID_PREFIX}-producer-{i}"
            self.producers[producer_id] = Producer(config)
            self.producer_usage[producer_id] = False

    def get_producer(self) -> Tuple[str, Producer]:
        """
        Get an available producer from the pool.
        If all producers are in use, wait for one to become available.

        Returns:
            Tuple of (producer_id, producer)

        Raises:
            TimeoutError: If no producer becomes available within the timeout period
        """
        with self.producer_condition:
            # First attempt: try to find an available producer
            for producer_id, in_use in self.producer_usage.items():
                if not in_use:
                    self.producer_usage[producer_id] = True
                    return producer_id, self.producers[producer_id]

            # If we have fewer than MAX_PRODUCERS, create a new one
            if len(self.producers) < self.__MAX_PRODUCERS:
                new_producer_id = f"producer-{len(self.producers)}"
                config = self.__KAFKA_CONFIG.copy()
                config["client.id"] = (
                    f"{self.__KAFKA_CLIENT_ID_PREFIX}-{new_producer_id}"
                )
                new_producer = Producer(config)
                self.producers[new_producer_id] = new_producer
                self.producer_usage[new_producer_id] = True
                logging.info(
                    f"Created new producer {new_producer_id} (total: {len(self.producers)}/{self.__MAX_PRODUCERS})"
                )
                return new_producer_id, new_producer

            # All producers are in use and we're at the limit, wait for one to become available
            logging.warning(
                f"All {len(self.producers)} Kafka producers are in use. Waiting for one to become available..."
            )

            # Wait for a producer to become available with timeout
            start_time = time.time()
            while True:
                # Wait for a notification that a producer might be available
                self.producer_condition.wait(timeout=self.__PRODUCER_ACQUIRE_TIMEOUT)

                # Check if any producer is now available
                for producer_id, in_use in self.producer_usage.items():
                    if not in_use:
                        self.producer_usage[producer_id] = True
                        logging.debug(f"Acquired producer {producer_id} after waiting")
                        return producer_id, self.producers[producer_id]

                # Check if we've exceeded our timeout
                if time.time() - start_time > self.__PRODUCER_ACQUIRE_TIMEOUT:
                    logging.error(
                        f"Timeout waiting for an available Kafka producer. All {len(self.producers)} producers are busy."
                    )
                    raise TimeoutError(
                        f"Timed out waiting for an available Kafka producer"
                    )

    def release_producer(self, producer_id: str):
        """
        Mark a producer as available and notify waiting threads

        Args:
            producer_id: ID of the producer to release
        """
        with self.producer_condition:
            if producer_id in self.producer_usage:
                self.producer_usage[producer_id] = False
                logging.debug(f"Released producer {producer_id}")
                # Notify one waiting thread that a producer is available
                self.producer_condition.notify()
            else:
                logging.warning(f"Attempted to release unknown producer: {producer_id}")

    def publish_message(self, key: str, value: dict) -> bool:
        """
        Publish a message to Kafka

        Args:
            key: Message key
            value: Message value (will be converted to JSON)

        Returns:
            True if message was sent successfully, False otherwise
        """
        producer_id = None
        try:
            producer_id, producer = self.get_producer()
            encoded_value = json.dumps(value).encode("utf-8")

            # Set up delivery tracking
            delivery_success = [False]
            delivery_complete = threading.Event()

            def delivery_callback(err, msg):
                if err:
                    logging.error(f"Message delivery failed: {err}")
                else:
                    logging.debug(
                        f"Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}"
                    )
                    delivery_success[0] = True

                # Release the producer for reuse
                self.release_producer(producer_id)
                delivery_complete.set()

            producer.produce(
                self.__KAFKA_PRODUCER_TOPIC,
                key=key,
                value=encoded_value,
                callback=delivery_callback,
            )

            # Trigger any callbacks and wait briefly for delivery
            producer.poll(0.1)

            # If delivery was immediate, wait for callback to complete
            if not delivery_complete.is_set():
                # Don't wait indefinitely - this is just to give a chance for immediate delivery
                delivery_complete.wait(0.5)

            # Return based on delivery result - if we didn't get a callback yet,
            # we'll still consider it "sent" since it's in Kafka's queue
            return True

        except Exception as e:
            logging.error(f"Failed to publish message to Kafka: {str(e)}")
            if producer_id:
                self.release_producer(producer_id)
            return False

    def flush_all(self):
        """Flush all producers"""
        for producer in self.producers.values():
            try:
                producer.flush(timeout=5.0)
            except Exception as e:
                logging.error(f"Error flushing producer: {e}")

    def close_all(self):
        """Close all producers"""
        with self.lock:
            for producer_id, producer in list(self.producers.items()):
                try:
                    producer.flush(timeout=5.0)
                    # Explicitly remove the producer
                    del self.producers[producer_id]
                    if producer_id in self.producer_usage:
                        del self.producer_usage[producer_id]
                except Exception as e:
                    logging.error(f"Error closing producer {producer_id}: {e}")

            # Clear the collections to ensure all references are removed
            self.producers.clear()
            self.producer_usage.clear()
            logging.info("All Kafka producers have been closed")

    def get_producer_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the producer pool

        Returns:
            Dictionary with producer pool statistics
        """
        with self.lock:
            total_producers = len(self.producers)
            active_producers = sum(
                1 for in_use in self.producer_usage.values() if in_use
            )
            return {
                "total_producers": total_producers,
                "active_producers": active_producers,
                "available_producers": total_producers - active_producers,
                "max_producers": self.__MAX_PRODUCERS,
            }
